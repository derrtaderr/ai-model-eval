name: LLM Model Evaluation

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - 'models/**'
      - 'prompts/**'
      - 'evaluation/**'
      - '.github/workflows/llm-evaluation.yml'
  push:
    branches: [ main, master ]
    paths:
      - 'models/**'
      - 'prompts/**'
      - 'evaluation/**'
  workflow_dispatch:
    inputs:
      evaluation_type:
        description: 'Type of evaluation to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - 'comprehensive'
          - 'regression'
          - 'performance'
          - 'safety'
      model_filter:
        description: 'Filter models to evaluate (comma-separated)'
        required: false
        type: string

env:
  EVALUATION_API_URL: ${{ secrets.EVALUATION_API_URL || 'http://localhost:8000' }}
  API_KEY: ${{ secrets.EVALUATION_API_KEY }}

jobs:
  detect-changes:
    name: Detect Model Changes
    runs-on: ubuntu-latest
    outputs:
      models-changed: ${{ steps.changes.outputs.models }}
      prompts-changed: ${{ steps.changes.outputs.prompts }}
      evaluation-changed: ${{ steps.changes.outputs.evaluation }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            models:
              - 'models/**'
              - 'config/models.yml'
            prompts:
              - 'prompts/**'
              - 'templates/**'
            evaluation:
              - 'evaluation/**'
              - 'tests/**'

  evaluate-models:
    name: Evaluate LLM Models
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.models-changed == 'true' || needs.detect-changes.outputs.prompts-changed == 'true' || github.event_name == 'workflow_dispatch'
    
    strategy:
      matrix:
        python-version: [3.9, 3.10]
        evaluation-type: ['safety', 'performance', 'accuracy']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r evaluation/requirements-dev.txt

      - name: Set up evaluation environment
        run: |
          mkdir -p evaluation/outputs
          mkdir -p evaluation/reports
          echo "EVALUATION_RUN_ID=gh-${{ github.run_id }}-${{ matrix.evaluation-type }}" >> $GITHUB_ENV

      - name: Run Model Evaluation - ${{ matrix.evaluation-type }}
        run: |
          python -m evaluation.runner \
            --type ${{ matrix.evaluation-type }} \
            --output-dir evaluation/outputs \
            --report-dir evaluation/reports \
            --run-id ${{ env.EVALUATION_RUN_ID }} \
            --webhook-url ${{ env.EVALUATION_API_URL }}/api/integrations/webhooks/github \
            --github-token ${{ secrets.GITHUB_TOKEN }} \
            --pr-number ${{ github.event.pull_request.number || '' }}

      - name: Upload evaluation results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: evaluation-results-${{ matrix.python-version }}-${{ matrix.evaluation-type }}
          path: |
            evaluation/outputs/
            evaluation/reports/
          retention-days: 30

      - name: Send results to evaluation platform
        if: always()
        run: |
          python -m evaluation.webhook_sender \
            --api-url ${{ env.EVALUATION_API_URL }} \
            --api-key ${{ env.API_KEY }} \
            --results-path evaluation/outputs \
            --github-context '${{ toJson(github) }}'

  regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [detect-changes, evaluate-models]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Download evaluation artifacts
        uses: actions/download-artifact@v3
        with:
          path: evaluation-results/

      - name: Run regression analysis
        run: |
          python -m evaluation.regression_checker \
            --baseline-branch main \
            --current-results evaluation-results/ \
            --threshold 0.05 \
            --output regression-report.json

      - name: Comment PR with results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            // Read regression report
            let report = {};
            try {
              const reportData = fs.readFileSync('regression-report.json', 'utf8');
              report = JSON.parse(reportData);
            } catch (error) {
              console.log('No regression report found');
              return;
            }
            
            // Create comment body
            let commentBody = '## ü§ñ LLM Evaluation Results\n\n';
            
            if (report.regression_detected) {
              commentBody += '‚ö†Ô∏è **Performance Regression Detected**\n\n';
              commentBody += '| Metric | Baseline | Current | Change |\n';
              commentBody += '|--------|----------|---------|--------|\n';
              
              for (const metric of report.regressed_metrics) {
                const change = ((metric.current - metric.baseline) / metric.baseline * 100).toFixed(2);
                commentBody += `| ${metric.name} | ${metric.baseline.toFixed(3)} | ${metric.current.toFixed(3)} | ${change}% |\n`;
              }
            } else {
              commentBody += '‚úÖ **No Performance Regression Detected**\n\n';
              commentBody += 'All evaluation metrics are within acceptable thresholds.\n';
            }
            
            commentBody += '\n### Evaluation Summary\n';
            commentBody += `- **Total Tests**: ${report.total_tests || 0}\n`;
            commentBody += `- **Passed**: ${report.passed_tests || 0}\n`;
            commentBody += `- **Failed**: ${report.failed_tests || 0}\n`;
            commentBody += `- **Evaluation Time**: ${report.total_time || 'N/A'}\n`;
            
            commentBody += '\n---\n';
            commentBody += `üîó [View detailed results](${process.env.EVALUATION_API_URL}/dashboard/run/${process.env.EVALUATION_RUN_ID})\n`;
            
            // Post comment
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

  export-data:
    name: Export Evaluation Data
    runs-on: ubuntu-latest
    needs: evaluate-models
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Export evaluation data
        run: |
          python -m evaluation.data_exporter \
            --api-url ${{ env.EVALUATION_API_URL }} \
            --api-key ${{ env.API_KEY }} \
            --format jsonl \
            --output-path exports/evaluation-data-$(date +%Y%m%d).jsonl \
            --time-range 24h

      - name: Upload to artifact storage
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-exports
          path: exports/
          retention-days: 90

      - name: Sync to S3 (optional)
        if: env.AWS_ACCESS_KEY_ID != ''
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          aws s3 sync exports/ s3://${{ secrets.EVALUATION_BUCKET }}/exports/$(date +%Y/%m/%d)/ 